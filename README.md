### Articula: AI-generated Content Detection and Understanding with Low-resource Training and Augmentations Resources

This repository presents implementation for the tool Articula: AI-generated Content Detection and Understanding with Low-resource Training and Augmentations. The Articula tool analyzes both input images and texts to detect whether they are generated by AI models.

### Developer Guidelines

1. For setting up the project, use the `source project-setup.sh` command, and it will set up a virtual environment with prerequisite packages.

### Pull Request Instructions

1. Create a specific branch named in the format like `feature/intrinsic-dimension-model`, highlighting the name of the feature model in progress and make a pull request.

2. Always `rebase` the your working branch before making a PR and add `--force-with-lease` based branch push. Also, we highlight the steps needed for making a PR below assuming that you are working on your feature branch and you can opt to remove conflicts via your IDE while doing the `rebase` for your branch.

```
git checkout main
git pull origin
git checkout feature/intrinsic-dimension-model
git rebase main
git push origin feature/intrinsic-dimension-model --force-with-lease
```

3. Finally, after reviewing the pull request, please `squash` all the commits during the merge into the main branch. Also, add in meaningful PR heading and joint-commit message for all your intermediate commits.

__Note:__ Please, do not push directly onto the main branch, follow the proposed conventions, and also add another person as reviewer of your code.

### Training and Testing the text detector

1. Extract the zip folders.
2. Generate the log probability files required for the training data.
```
python common/ghostbusters_generate.py --logprobs
python common/ghostbusters_generate.py --logprob_other
```
3. Generate the symbolic data which will be used to perform feature selection
```
python trainer/ghostbuster_feature_selection.py --generate_symbolic_data
```
4. Perform Feature Selection
```
python trainer/ghostbuster_feature_selection.py --perform_feature_selection
```
5. Train the classifier (Logistic, XGBoost or Random Forest)
```
python trainer/ghostbuster-train.py --model_name "logistic" --combined
```
6. Use the app/text_app.py to test and spoof
```
python app/text_app.py --input "your text"
```

## 1. Dataset
The experimentation utilizes the bensvmark datasets employed by previous state-of-the-art (sota) models. In this project, a custom model was trained on a subset of images extracted from all these models, creating a diverse dataset.

## 2. Ensemble Setup
We adopted an ensemble setup for our experimentation. The models can be trained using the following command:
```
cd trainer
python img-train.py --name test --dataroot [your data path] --detect_method [CNNSpot, FreDect, Fusing, Gram] --blur_prob 0.1 --blur_sig 0.0,3.0 --jpg_prob 0.1 --jpg_method cv2,pil --jpg_qual 30,100  
```

Alternatively, already trained models are available at this drive

## 3. Training the Ensemble Setup
The ensemble setup utilizes CNNSpot, FreDect, Fusing, and Gram models. Once the pretrained models are ready and placed under the weights folder, run the following to train the ensemble meta_model:
```
cd trainer
python ensemble-train.py
```
## 4.Testing Model
Testing the model can be done using the following script:

```
cd evaluator
python ensemble_prediction_pipeline.py
```
Note: For running all these scripts and understanding the arguments for training/testing the model, refer to the file common/img_options.py and the config file evaluator/configs/img_eval_config.py.

### Testing with Spoofing

Multiple strategies are experimented with, such as Gaussian attack, salt and pepper noise attack, and frequency edits. These can be experimented with using the same ensemble_prediction_pipeline.py file by changing the options. For example:
```
python ensemble_prediction_pipeline.py --gaussian_noise --gaus_intensity 5
python ensemble_prediction_pipeline.py --salt_pepper_noise --s_p_density 0.1
python ensemble_prediction_pipeline.py --freq --freq_prob 0.1
```
same can be run using the following script
```
cd evaluator/eval_different_attacks.sh
./eval_different_attacks.sh
````
